%!TEX root = ../dokumentation.tex

\newtheorem{definition}{Definition}[section]


\chapter{Theoretische Hintergründe}

Vor dem Erstellen einer Schach-\acs{KI} (\acl{KI}) muss zunächst betrachtet werden, wie der Aufbau von KIs für spiel-mechanische Abläufe ist, welche Funktionen diese enthalten müssen und wie diese speziell auf das Spiel ``Schach'' angewandt werden können. Dazu werden zunächst Grundlagen der Spieltheorie betrachtet ehe näher auf Möglichkeiten zur Nutzung dieser im Kontext des Schachspiels eingegangen wird. Im zweiten Teil werden die verwendeten Methoden für die in dieser Arbeit beschriebene KI erläutert und die Wahl begründet.

\section{Spieltheorie}

\subsection{Geschichte}

Die Geschichte der Spieltheorie im Computerumfeld beginnt mit Claude Shannon im Jahre 1949, als dieser seine Gedanken zur möglichen Realisierung eines Schach spielenden Computers veröffentlicht. Dabei begründet er zunächst seine Auswahl auf das Spiel Schach für sein langfristiges Ziel eines spielenden Computers und legt dann das Ziel an sich fest.

\begin{quote}
“The chess machine is an ideal one to start with, since: (1) the problem is sharply defined both in allowed operations (the moves) and in the ultimate goal (checkmate); (2) it is neither so simple as to be trivial nor too difficult for satisfactory solution; (3) chess is generally considered to require ‘thinking’ for skillful play; a solution of this problem will force us either to admit the possibility of a mechanized thinking or to further restrict our concept of ‘thinking’; (4) the discrete structure of chess fits well into the digital nature of modern computers. … It is clear then that the problem is not that of designing a machine to play perfect chess (which is quite impractical) nor one which merely plays legal chess (which is trivial). We would like to play a skillful game, perhaps comparable to that of a good human player.”
\end{quote}

Die wesentlichen Punkte von Claude Shannon sind dabei, dass Schach weder zu simpel noch zu komplex ist, um es von einem Computer spielen zu lassen - Es gibt ein eindeutiges Ziel und eindeutige Operationen auf dem Weg zu diesem Ziel, aber kein eindeutig perfektes Spiel. Dementsprechend kann weder dies das Ziel sein, noch kann es das Ziel sein, einfach nur ein legales Spiel durchzuführen. Viel mehr muss es mit dem Schachspiel von Menschen vergleichbar sein. Menschen kennen den perfekten Zug nicht, können jedoch eine begrenzte Zahl an Zügen in dessen Wahrnehmungsbereich evaluieren und den in diesem Rahmen scheinbar besten Zug auswählen. Ein ähnliches Verhalten sollte auch bei einem Computer erkennbar sein.

Der vorgeschlagene Ansatz zum Evaluieren der Züge stellt dabei ein Algorithmus dar, der heute unter ``Minimax-Algorithmus'' bekannt ist. Dieser Ansatz wird in Kapitel 2.1.2 erklärt.

%http://www.andreykurenkov.com/writing/ai/a-brief-history-of-game-ai/

Anwendung fand dies erstmals im Jahre 1956, als ein Team von Mathematikern und Forschern um Arthur Samuel ein Programm entwickelten, das in der Lage war ``Dame'' zu spielen. Dies war das erste mal, das ein Computer in der Lage war in einem strategischen Spiel gegen einen Menschen anzutreten. Im Rahmen dieses Projekts ist auch der Begriff ``Artificial Intelligence'' oder zu deutsch ``Künstliche Intelligenz'' entstanden.

Dabei wird vom aktuellen Zustand aus einige Züge voraus geschaut und bewertet, welcher Zug unter der Annahme, dass auch der Gegner den jeweils besten Zug machen wird, der aussichtsreichste ist. Zur Berechnung wird ein Minimax Baum ausgehend von der aktuellen Spielsituation aus erstellt.

%image?

Auch wurden hier erste Machine-learning Algorithmen verwendet, die dem Computer innerhalb kürzester Zeit das Spiel so gut beibringen, dass dieser dazu in der Lage ist menschliche Spieler zu schlagen. Samuel verwendete dazu zwei Methoden - Einerseits ``rote-learning'', das die Werte von bestimmten Spielzuständen, die bereits evaluiert wurden, abspeichert, so dass die Berechnung kein weiteres mal durchgeführt werden muss. Zum Anderen ``learning-by-generalization'', das die Parameter der Evaluierungsfunktionen basierend auf vorherigen Spielen anpasst. Dies geschah mit dem Ziel, den Unterschied zwischen dem berechneten Evaluierungswert des Zustands und dem tatsächlichen, auf den Ausgang des jeweiligen Spiels basierenden Werts zu minimieren.

%https://www.cs.virginia.edu/~evans/greatworks/samuel1959.pdf

Nur ein Jahr später fand die Theorie von Claude Shannon auch erstmals direkt im Spiel Schach Anwendung. Ein Team um Mathematiker Alex Bernstein entwickelte eine voll funktionale Schach spielende KI, ebenfalls basierend auf den Minimax Algorithmen. Die Problematik beim Schach im Vergleich zu anderen Spielen sind die enorme Zahl an verschiedenen Zügen und Spielzuständen, die eine Evaluierung jedes einzelnen selbst bei heutiger Rechenkapazität unmöglich macht. Dies ist genauer in Kapitel 2.1.4 beschrieben.

Auf Grund dieser Komplexität des Schach Spiels sowie der begrenzten Rechenkapazitäten der damaligen Computer jedoch wurde dies auf eine Tiefe von 4 begrenzt. Dies bedeutet, dass der Computer lediglich 4 Züge vorausschaut. Zusätzlich schaut der Computer lediglich 7 verschiedene Optionen pro Zug an. Die Auswahl dieser 7 Optionen geschah mittels simpler heuristischer Berechnungen, die versuchten im Vorab die 7 aussichtsreichsten Optionen zu wählen.

%Minimax Tree image?

Diese Limitationen jedoch ermöglichten lediglich ein relativ simples, wenn auch passables Schachspiel.

%https://d1yx3ys82bpsa0.cloudfront.net/chess/computer-v-chessplayer.bernstein-roberts.scientific-american.june-1958.062303059.pdf

Um die Zahl der zu evaluierenden Züge zu reduzieren, wurde im Jahr 1958 von Allen Newell und Herbert Simon eine Modifizierung des Minimax Ansatzes veröffentlicht - genannt ``alpha beta pruning''. Diese Modifizierung verhindert das Evaluieren von Zügen, die eindeutig schlechter sind. Dieser Ansatz wird genauer in Kapitel 2.1.3 beschrieben.

%Allen Newell, Cliff Shaw, Herbert Simon (1958). Chess Playing Programs and the Problem of Complexity. IBM Journal of Research and Development, Vol. 4, No. 2, pp. 320-335. Reprinted (1963) in Computers and Thought (eds. Edward Feigenbaum and Julian Feldman), pp. 39-70. McGraw-Hill, New York, N.Y. pdf

Diese neue Vorgehensweise kann einiges an Rechenkapazitäten sparen und soll so zum ersten Mal einen menschlichen Spieler geschlagen haben, der das Spiel allerdings erst kurz zuvor erlernt hatte.

Durch Verbesserung der Algorithmen und erweiterten Rechenkapazitäten durch immer bessere und performantere Computer, teilweise extra optimiert auf das Schach Spiel, konnten Stück für Stück neue Erfolge erzielt werden. 1962 konnte eine KI von Arthur Samuel erstmals einen renommierten Spieler, Robert Nealy, schlagen.

1994 konnten die besten Dame Spieler der Welt eine KI namens ``CHINOOK'' nur noch ein Unentschieden abverlangen. Bereits 1988 gelang einem Programm namens ``Deep Thought'' von Feng-hsiung Hsu, später von IBM weiterentwickelt unter dem Namen ``Deep Blue'' bekannt, ähnliches im Schach Spiel und so schlug die KI den Schach Weltmeister.

Beide Programme basierten dabei auf drei wesentlichen Punkten: Eine Datenbank von Eröffnungszügen entnommen von professionellen Spielern, alpha-beta Suchbäume mit einer Menge an Evaluierungsfunktionen sowie einer Endspiel Datenbank sobald nur noch eine geringe Anzahl an Spielfiguren existiert.

%http://www.andreykurenkov.com/writing/ai/a-brief-history-of-game-ai-part-2/

Diese Programme benötigten speziell optimierte Computer, die auf die nötigen Berechnungen für die jeweiligen Spiele ausgelegt waren. Anders handhabt es das Programm ``Stockfish'' aus dem Jahre 2008. Dies ist auf jedem Computer ausführbar und dennoch für einen Menschen scheinbar nicht schlagbar.

Lange war Stockfish unbesiegt, dies änderte sich jedoch im Jahr 2017 als AlphaZero mit 64:36 gegen Stockfish gewann. AlphaZero ist dabei ein verallgemeinerter Ansatz von AlphaGo Zero. Dies wiederum ist eine Aktualisierung von AlphaGo, das im Jahr 2016 den Weltmeister in Go schlug. Dabei verwendet AlphaGo neuronale Netzwerke und verwendet Methoden des ``supervised learning'' und ``reinforcment learning'', um sich selbstständig weiter zu entwickeln.

%https://de.wikipedia.org/wiki/Stockfish
%https://de.wikipedia.org/wiki/AlphaZero
%Quellen aus Wikipedia suchen

An diesem Punkt endet die Entwicklung von immer besser werdenden KIs jedoch nicht. Besonders durch den Fortschritt im Bereich der neuronalen Netze und speziell im ``Deep Learning'' Bereich werden immer komplexere Spiele durch den Computer beherrscht, immer öfter auch besser als vom Menschen. Ein Beispiel stellt dabei Alpha Star dar, das im Jahre 2019 einer der besten Teams im Echtzeit-Strategiespiel ``Starcraft 2'' geschlagen hat. Diese Entwicklung wird sich wohl auch in den kommenden Jahren fortsetzen, auch außerhalb der Spiele Szene, weshalb KI in unserer Geselschaft immer mehr an Bedeutung gewinnt.

%Quelle

%first check steps
%alpha beta
%deep blue
%stockfish



\subsection{Minimax-Algorithmus}

%Theorem
%Erklärung
%Chess minimax tree
%Problematik

Der Minimax Algorithmus basiert auf dem ``Minimax-Theorem''  von John Vonn Neumann.

Der Hauptsatz des Theorems für 2 Personen Spiele lautet:

\begin{quote}
In der gemischten Erweiterung $(X, Y, G')$ eines jeden 2-Personen-Nullsummenspiels mit endlichen (reinen) Strategieräumen A und B existiert eine Konstante V und für jeden Spieler mindestens eine (gemischte) Gleichgewichtsstrategie $x^*$ bzw. $y^*$, mit der er eine erwartete Auszahlung von mindestens V garantieren kann.

Für Spieler A existiert ein $x^*$ = ${x^*_1, ... x^*_i, ... x^*_m}$ mit $x^*_i \geq 0$ und $\sum_{i = 1}^m x^*_i= 1\quad $, so dass $\quad \max\limits_x$ $\min\limits_y$ $G^\prime\bigl(x,y\bigr)\ = \min\limits_y$ $G^\prime\bigl(x^*,y\bigr) = V$.

Für Spieler B existiert ein $y^* = \{ y^*_1, ... y^*_j, ... y^*_n\}$ mit $y^*_j\ge 0 $ und $\sum_{j = 1}^n y^*_j= 1\quad$, 
so dass $\quad \min\limits_y$ $\max\limits_x$ $G^\prime\bigl(x,y\bigr)\ = \max\limits_x$ $G^\prime\bigl(x,y^*\bigr) = V$.
\textsuperscript{\cite{}}
\end{quote}
% Hans Bühlmann, Hans Loeffel, Erwin Nievergelt: Entscheidungs- und Spieltheorie, Springer Verlag, Berlin , 1975, S. 183. (wikipedia)

Dabei wird das Spiel von Spieler A gestartet, der somit zuerst eine Strategie wählt. Die Annahme dabei ist, dass Spieler A davon ausgeht, dass Spieler B stets den für ihn best möglichen Zug spielen wird. Das bedeutet, dass Spieler A die Strategie wählt, bei der unter dieser Annahme dennoch das beste Endergebnis zu erreichen ist. Hat Spieler A also eine Menge $S$ an Strategien zur Auswahl, werden alle Szenarien von jedem Zug $s \in S$ evaluiert. Die Bewertung dieser Strategien nennen wir $u(s) | s \in S$. Diese Evaluation erfolgt dabei aus Perspektive von Spieler A. Dies bedeuet, dass umso höher das Ergebnis von u(s) ist, desto besser ist das Ergebnis für Spieler A. Je niedriger das Ergebnis, desto besser für Spieler B.

Dann wird das Minimum aller Bewertungen der Endzustände für jede Strategie $s$ gewählt. Diese Strategien nennen wir $min(s)$ und dessen Bewertung somit $u(min(s))$, wobei gilt $s \in S$. Nun wird die Strategie $s_{best} \in S$ gewählt, so dass gilt:

$ \forall s \in S : (s \neq s_{best} \rightarrow u(min(s)) \leq u(min(s_{best})))$

Anders ausgedrückt - existiert also eine Menge X von Strategien von Spieler A und eine auf X konvexe Menge Y von möglichen Strategien von Spieler B, so lautet die Optimierungsregel für Spieler A

$\max\limits_X[\min\limits_Yu(X,Y)]$

Umgekehrt versucht Spieler B das für Spieler A ungünstigste Ergebnis zu wählen und wählt daher die Strategie, die das Minimum der für Spieler A jeweils nach dem Zug noch bestmöglichen Ergebnisse versprechen. Somit lautet die Optimierungsregel für Spieler B

$\min\limits_Y[\max\limits_Xu(X,Y)]$

%quelle (https://de.wikipedia.org/wiki/Min-Max-Theorem#cite_note-9)

Dadurch kann Spieler B den Ertrag des Spielers A auf diesen Wert begrenzen. Es gilt also

$\max\limits_X[\min\limits_Yu(X,Y)] \leq \min\limits_Y[\max\limits_Xu(X,Y)]$

Das Theorem geht dabei davon aus, dass es also einen Sattelpunkt $v$ geben muss, bei dem sich die beiden Optimierungen für Spieler A und B einpendeln. Dieser Sattelpunkt lautet

$\max\limits_X[\min\limits_Yu(X,Y)] = \min\limits_Y[\max\limits_Xu(X,Y)] = v$

Diese Strategie basiert auf puren Berechnungen und ist für einen Computer somit leicht durchführbar. Dafür verlangt es folgende Informationen:

\begin{itemize}
\item \textbf{$States$}: Alle möglichen Zustände des Spiels
\item \textbf{$s_0$}: Anfangszustand des zu betrachtenden Spiels
\item \textbf{$player(s)$}: Gibt den Spieler zurück, der in gegebenem Zustand am Zug ist
\item \textbf{$actions(s)$}: Eine Liste aller möglichen Züge ausgehend von Zustand s
\item \textbf{$restultState(s,a)$}: Der von Zustand s über Zug a erreichbare Zustand
\item \textbf{$terminalTest(s)$}:  Prüft einen Zustand darauf, ob dieser das Ende des Spiels bedeutet. Diese wird definiert als

$terminalTest : States \rightarrow \mathbb{B}$

wobei $\mathbb{B}$ einem Boolean-Wert, sprich wahr oder falsch, entspricht.

Mittels dieser Funktion kann eine Menge $terminalStates$ gebildet als Menge aller Endzustände gebildet werden:

$terminalStates := {s \in S | terminalTest(s)}$
\item \textbf{$utility(s, p)$}: Bewertet einen Zustand, indem sie diesem einen numerischen Wert zuweist. Diese Funktion ist definiert durch

$utility : terminalStates x player \rightarrow \mathbb{N}$

wobei diese den Zustand aus Sicht des gegebenen Spielers bewertet. Umso höher die Zahl N also, desto besser das Ergebnis für Spieler P.
\end{itemize}

Um die erreichbaren Zustände zu erhalten, benötigt es einen Algorithmus, der an Hand der Spielregeln und eines Ausgangszustands $s_0$ und einer Liste aller Züge von s $actions(s)$ alle erreichbaren Zustände $resultStates$ berechnet. Von jedem Zustand $s \in resultStates$ wird dann wiederum jeder mögliche erreichbare Zustand berechnet. Diese Schleife wird fortgeführt, bis die berechneten Zustände den Status eines Endzustandes erreichen, also $terminalTest(s) = true$ ergeben, von dem aus keine Änderungen des Zustands mehr möglich sind.

Entscheidend für die Wahl eines Zuges ist dann die Bewertung jedes einzelnen Zustandes. Dies ist solange umsetzbar, wie der Computer ohne Probleme alle möglichen Zustände berechnen und evaluieren kann. Um dies an einem Beispiel zu zeigen - Beim Spiel ``Tic Tac Toe'', bei dem in einem 3x3 Feld zwei Spieler gegeneinander antreten mit dem Ziel drei aneinander angrenzende (vertikal, horizontal oder diagonal) Felder mit ihrer Figur zu belegen, gibt es insgesamt 255.168 verschiedene Spielverläufe. Diese sind von heutigen Computern in akzeptabler Zeit berechenbar und evaluierbar.

%https://books.google.de/books?id=mUIzDwAAQBAJ&pg=PT18&lpg=PT18&dq=tic+tac+toe+m%C3%B6gliche+255+168&source=bl&ots=CcuZA068gP&sig=ACfU3U0XQ053sl4-BcVsL-hPnm76l1ZHyg&hl=de&sa=X&ved=2ahUKEwiL_snW6oLhAhXBIVAKHc7tD40Q6AEwAHoECAcQAQ#v=onepage&q=tic%20tac%20toe%20m%C3%B6gliche%20255%20168&f=false

Dazu bildet der Computer einen sogenannten Minimax-Baum und wählt dann den erfolgversprechendsten Zug aus. Nach jedem getätigten Zug werden die erreichbaren Zustände nur noch vom neuen Zustand aus berechnet. Im Spätspiel kann ein solcher Baum bei ``Tic Tac Toe'' beispielsweise aussehen wie in Figur 2.2.1.1

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth/5*3]{images/tictactoe_minimax_tree.png}

\textsuperscript{Figure 2.1.1.1 Tic Tac Toe Minimax Baum \cite{}}\\
\end{figure}
%http://www.andreykurenkov.com/writing/ai/a-brief-history-of-game-ai/

Dabei werden vom Ausgangszustand, der den Zustand des aktuellen Spiels widerspiegelt, aus alle möglichen Folgezustände berechnet. Von diesen werden wiederum alle möglichen Folgezustände berechnet. Dies wird solange fortgeführt, bis alle neu berechneten Zustände Endzustände sind. Dann werden alle Endzustände evaluiert. Eine 1 stellt dabei einen Sieg dar, eine 0 ein Unentschieden und eine -1 eine Niederlage. Da davon ausgegangen wird, dass der gegnerische Spieler stets den besten Zug auswählt, wird jeder Zustand, der noch Folgezustände besitzt, mit dem für ihn schlechtesten Wert aller seiner möglichen Folgezustände bewertet. Der Computer wählt dann den Zustand mit der für ihn besten Bewertung.
%zu minimax?

In diesem Beispiel wird sich der Computer somit für den dritten Zug von links entscheiden, da bei beiden anderen ein Sieg des Gegenspielers bevorsteht, sollte dieser jeweils die perfekten Züge spielen. Beim dritten Zug kann der Gegner maximal noch ein Unentschieden erreichen.

Diese Strategie gerät jedoch dann an ihre Grenzen, wenn nicht mehr alle möglichen Zustände berechnet werden können. Bei dem Spiel Schach beispielsweise belaufen sich Schätzungen schon nach den ersten 40 Spielzügen auf 10\textsuperscript{115} - 10\textsuperscript{120} verschiedene Spielverläufe. Dies ist auch für einen Computer in tolerierbarer  Zeit unmöglich berechenbar. Aus diesem Grund muss die Strategie für solch komplexere Spiele abgewandelt werden. Die verschiedenen Ansätze dazu sind in Kapitel 2.1.4 beschrieben.

% Eero Bonsdorff, Karl Fabel, Olavi Riihimaa: Schach und Zahl. 3. Auflage, Rau, Düsseldorf 1978.
%https://de.wikipedia.org/wiki/Schach#cite_note-6

Um diesen Algorithmus in die Praxis umzusetzen, verlangt es drei Funktionen, die jeweils auf den Parameter \textbf{state} angewiesen sind. Dieser Parameter gibt Aufschluss über den aktuellen Zustand des Spiels.

Zusätzlich verlangt der Algorithmus neben der bei \ref{} genannten Funktionen $utility$ und $finished$ eine Funktion $min$ sowie eine Funktion $max$, die jeweils mehrere Werte vergleichen und den Minimum bzw. Maximum aller verglichenen Werte zurück geben. Die Implementierung eines solchen mittels drei verschiedener Funktionen kann dann wie folgt aussehen:

\begin{lstlisting}[mathescape]
function minimaxDecision(state) returns state
	for each a in actions(state) do
		value $\leftarrow$ minValue(restultState(state, a))
		if value > bestValue then
			bestValue $\leftarrow$ value
			bestMove $\leftarrow$ a
	return bestMove
	
function maxValue(state) returns value
	if finished(state) then return utility(state)
	v $\leftarrow$ -infinity
	for each a in actions(state) do
		v $\leftarrow$ max(v, minValue(restultState(state, a)))
	return v
	
function minValue(state) returns value
	if finished(state) then return utility(state)
	v $\leftarrow$ infinity
	for each a in actions(state) do
		v $\leftarrow$ min(v, maxValue(restultState(state, a)))
	return v
\end{lstlisting}

Die erste Funktion geht dabei jeden möglichen Zug vom gegebenen Ursprungszustand durch und gibt am Ende den Zustand zurück, der den besten Minimum-Wert durch die Evaluierungsfunktion $utility$ erreicht. Dazu gibt die Funktion jeden der erreichbaren Zustände zu der minValue Funktion. Diese gibt entweder den durch die $utility$ Funktion errechneten Wert des Zustands zurück, sollte der Zustand ein Endzustand sein, oder sie gibt das Minimum aller Maxima-Werte der erreichbaren Zustände zurück. Dazu wiederum wird die maxValue Funktion verwendet, die genau das Gegenteil der minValue Funktion macht. Ist der Endzustand erreicht, gibt zwar auch die maxValue Funktion den Wert des Zustands direkt zurück, andernfalls aber gibt sie das Maximum aller Minimum Werte der vom gegebenen Zustand aus erreichbaren Zustände zurück, wozu wiederum auf die minValue Funktion zurückgegriffen wird. Dies ganze wiederholt sich also rekursiv so lange, bis alle neu errechneten Zustände den Wert eines Endzustandes erreicht haben.

Diesen Endzuständen werden dann Werte zugewiesen, die aufsteigend rekursiv miteinander verglichen und abwechselnd Minimum und Maximum gewählt werden, um dem Minimax-Theorem dahingehend zu folgen. Um das ganze zu verdeutlichen kann Figur 2.1.2.1 als Beispiel genommen werden:

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth/5*3]{images/minimax_algorithm_tree.png}

\textsuperscript{Figure 2.1.2.1 Minimax-Baum \cite{}}\\
\end{figure}

Dabei hat der gegebene Zustand noch eine erreichbare Tiefe von drei bis alle darauf folgenden Zustände Endzustände sind. Diesen Endzuständen wird dann ein Wert zugewiesen. Nun werden diese Werte auf einer um eins höheren Ebene verglichen. Da es sich hierbei um eine gerade Tiefe (2) handelt, wird hier der Minimum dieser Werte gewählt. Im Knoten B ist dies in unserem Beispiel 3, das 3 der geringste Wert der Endzustände (3, 12 und 8) ist. Im Knoten B sowie D ist jeweils 2 der geringst mögliche Wert. 

Von diesen ausgerechneten Minimas wird dann auf Ebene 1 das Maximum berechnet. Das Maximum von 3, 2 und 2 beträgt 3, weshalb der Wert von Knoten B der höchste ist und somit wird der Zug, der zu Zustand B führt, zurück gegeben.

Somit ist es mit dem Minimax-Algorithmus möglich an Hand für den Computer möglicher Berechnungen ein Spiel zu evaluieren und den besten Zug zu wählen unter der Annahme, dass der Gegenspieler ebenso handeln wird. Problematisch dabei ist jedoch besonders bei komplexeren Spielen die Dauer der Evaluierung aller Züge. Dies liegt zum Einen daran, dass jeder einzelne Zug evaluiert wird und somit bei komplexen Spielen eine enorm hohe Zahl an Zuständen evaluiert werden muss. Zum Anderen ist das größte Problem aber wohl, dass es zwangweise das Spiel bis in die Endzustände berechnen muss, was eine enorm hohe Tiefe verlangt und somit eine extreme Zeitspanne zum Berechnen mit sich zieht. So ist die Zeitkomplexität bei einem Spiel der Tiefe m und einer Anzahl m an möglichen Zügen gleich $O(b^m)$.


%genaue Zahl?

Das erste Problem, dass jeder einzelne Zug berechnet wird, wird mit dem Ansatz des Alpha Beta Pruning versucht zu minimieren. Dieser Ansatz wird im Kapitel 2.1.3 erklärt. Das Problem, dass stets bis in die Endzustände gerechnet werden muss, wird in Kapitel 2.1.4 genauer erläutert sowie einige Lösungsansätze dargestellt.

% bessre Überleitung formuliere


%algorithmus in praxis

\subsection{Alpha-beta pruning}



\subsection{Problematik Minimax-Algorithmen komplexerer Spiele}



% Einleitung: Alle Züge evaluierbar
% Evaluerungsmöglichkeiten
   % board wert (Figuren)
   % attackierte figuren
   % board wert (Figuren + Positionen)
   % History
   % Wichtige Figuren attackierbar => ---
   % abrubtes ende wenn bestimmter zug spiel beendet (finishing - wenn bauer zu schlagen spiel beendet dann sollte man das machen)
% Überleitung: Nicht mehr alle evaluierbar => Iterative Deepening
% Methoden: Bestimmte Tiefe / Bestimmte Zeit
% Minimax
%Improvement: Alpha Beta
% Opening book
   % wozu
% Finishing book/strategy?
   % wozu (ewige jagd; attackierte figur von einer seite genauso viel wert wie von mehreren seiten)
   
   
% andere attackierte figuren evaluation?
% defense evaluation
% controlling evaluation?



\section{Evaluierungsfunktionen}

\subsection{Eröffnungsstrategie}

\subsection{Figurenbewertung (+ Position)}

\subsection{Verteidigung}

\subsection{Angriff}

\subsection{Spielverlauf}

\subsection{Finishing strategy}
